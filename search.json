[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/local-llms/index.html",
    "href": "posts/local-llms/index.html",
    "title": "Setting up local LLMs using ollama",
    "section": "",
    "text": "Brief overview\nProjects\nGithub\n(very brief) Quarto/this blog"
  },
  {
    "objectID": "posts/local-llms/index.html#getting-started-in-rstudio",
    "href": "posts/local-llms/index.html#getting-started-in-rstudio",
    "title": "Setting up local LLMs using ollama",
    "section": "",
    "text": "Brief overview\nProjects\nGithub\n(very brief) Quarto/this blog"
  },
  {
    "objectID": "posts/local-llms/index.html#installing-local-llms",
    "href": "posts/local-llms/index.html#installing-local-llms",
    "title": "Setting up local LLMs using ollama",
    "section": "Installing local LLMs",
    "text": "Installing local LLMs\n\n\n\n\n\n\nOllama\n\nDownload ollama and install (available for Mac, Linux, and Windows).\n\nhttps://ollama.com/\n\nRun ollama for the first time, you will see a llama icon in your menu bar.\nNow you can run your first test with:\nollama run llama3\nThis will download the model llama3 and start an interactive chat session.\nWhen you are done, end chat with \\bye\nYou can download many other models using this terminal command:\n\n\n\n\n\n\n\n\n\nModel\nParameters\nSize\nDownload\n\n\n\n\nLlama 3\n8B\n4.7GB\nollama run llama3\n\n\nLlama 3\n70B\n40GB\nollama run llama3:70b\n\n\nPhi 3 Mini\n3.8B\n2.3GB\nollama run phi3\n\n\nPhi 3 Medium\n14B\n7.9GB\nollama run phi3:medium\n\n\nGemma\n2B\n1.4GB\nollama run gemma:2b\n\n\nGemma\n7B\n4.8GB\nollama run gemma:7b\n\n\nMistral\n7B\n4.1GB\nollama run mistral\n\n\nMoondream 2\n1.4B\n829MB\nollama run moondream\n\n\nNeural Chat\n7B\n4.1GB\nollama run neural-chat\n\n\nStarling\n7B\n4.1GB\nollama run starling-lm\n\n\nCode Llama\n7B\n3.8GB\nollama run codellama\n\n\nLlama 2 Uncensored\n7B\n3.8GB\nollama run llama2-uncensored\n\n\nLLaVA\n7B\n4.5GB\nollama run llava\n\n\nSolar\n10.7B\n6.1GB\nollama run solar\n\n\n\n\n\nNote: You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n\n\n\nOpen WebUI graphical chat app for ollama models\nIt would be nicer if we had a GUI chat interface to interact with the model and could get code responses in syntax-highlighted code blocks. We can do that and more by installing Open WebUI.\nOpen WebUI can also be used with non-local models like ChatGPT and Github Copilot, but we will focus on local LLMs for this tutorial.\nOpen WebUI: https://github.com/open-webui/open-webui\n\nYou will need to have docker installed. The easiest method is to download and install Docker Desktop from here: https://www.docker.com/products/docker-desktop/\n\nDocker Desktop is free and can be used for commercial use for companies up to 250 employees OR $10 million in yearly revenue.\n\nOnce Docker Desktop is installed, run it. This will enable you to use the docker command from the terminal.\n\nThere are other ways to install docker, you just need to be able to run docker and docker compose commands.\n\nInstall and run Open WebUI with the following command:\ndocker run -d -p 3000:8080 –add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data –name open-webui –restart always ghcr.io/open-webui/open-webui:main\nIn a web browser go to http://localhost:3000\n\nOpen WebUI will load\nYou can install additional models from the menu\n\nInstall llama3:instruct, we will need it later.\n\nYou can save conversation chains\nYou can select multiple models and ask a question and it will select the best suited model to answer your question.\n\n(bonus) For anyone running MacOS 13 or higher, open the link in Safari and from the menu bar, choose File &gt; Add to Dock. This will create a standalone webapp.\n\nMore info here: https://support.apple.com/en-us/104996"
  },
  {
    "objectID": "posts/local-llms/index.html#installing-danswer-for-rag-with-ollama-models",
    "href": "posts/local-llms/index.html#installing-danswer-for-rag-with-ollama-models",
    "title": "Setting up local LLMs using ollama",
    "section": "Installing Danswer for RAG with ollama models",
    "text": "Installing Danswer for RAG with ollama models\nRAG stands for Retrieval-Augmented Generation. So far our models can answer questions and are great for giving code examples, but the model cannot access a web page or pull information from local documents. With Danswer, we can allow ollama models to do exactly that.\nDanswer: https://www.danswer.ai/\nDanswer Github: https://github.com/danswer-ai/danswer\nThis install is a little more involved than the last.\n\nGo to the Danswer website (https://www.danswer.ai/) and click “Self-host for Free.”\n\nThis will take you here: https://docs.danswer.dev/quickstart\nWe will follow the instructions on that page (and then make some changes)\n\nClone the Danswer repo:\ngit clone https://github.com/danswer-ai/danswer.git\nNavigate to danswer/deployment/docker_compose\ncd danswer/deployment/docker_compose\nEdit the file danswer/deployment/docker_compose/docker-compose.dev.yml\n\nFind \"3000:80\" and change it to \"3001:80\"\nThis will make the webapp available at http://localhost:3001 since we are already using port 3000 for Open WebUI\n\nTo connect danswer to ollama, create a file called .env in the danswer/deployment/docker_compose directory:\ntouch .env\n\nAdd the following to the file:\n\nGEN_AI_MODEL_PROVIDER=ollama_chat\n# Model of your choice\nGEN_AI_MODEL_VERSION=llama3:instruct\n# Wherever Ollama is running\n# Hint: To point Docker containers to http://localhost:11434, use host.docker.internal instead of localhost\nGEN_AI_API_ENDPOINT=http://host.docker.internal:11434\n\n# Let's also make some changes to accommodate the weaker locally hosted LLM\nQA_TIMEOUT=120  # Set a longer timeout, running models on CPU can be slow\n# Always run search, never skip\nDISABLE_LLM_CHOOSE_SEARCH=True\n# Don't use LLM for reranking, the prompts aren't properly tuned for these models\nDISABLE_LLM_CHUNK_FILTER=True\n# Don't try to rephrase the user query, the prompts aren't properly tuned for these models\nDISABLE_LLM_QUERY_REPHRASE=True\n# Don't use LLM to automatically discover time/source filters\nDISABLE_LLM_FILTER_EXTRACTION=True\n# Uncomment this one if you find that the model is struggling (slow or distracted by too many docs)\n# Use only 1 section from the documents and do not require quotes\nQA_PROMPT_OVERRIDE=weak\n\nIMPORTANT: Add an empty return at the end. The final empty line at the end appears important.\n\nMake sure Docker Desktop is running.\nTo start the application run: (run this from within danswer/deployment/docker_compose)\ndocker compose -f docker-compose.dev.yml -p danswer-stack up -d --pull always --force-recreate\n\nThis step may take a little while to download everything and install.\n\nGo to http://localhost:3001 to load the webapp\n\nYou will need to set up the app before you can start using it:\n\nGo to “Search / Chat with Knowledge” &gt; “Get started”\nSelect the “Custom” tab\nFill in the following values: (none of these are optional even if it say it is)\n\nDisplay Name: ollama\nProvide Name: ollama\nAPI Base: http://host.docker.internal:11434\nModel Names:\n\nllama3:instruct\nllama3:latest\n(you can add more here if you have more installed)\n\nDefault Model: llama3:instruct\n\nClick “Test” to see if all of your settings are correct.\n\nIf it’s successful, you may be good to go\nYou will know for sure once you test running a chat.\n\nClick “Enable”\nClick “Setup your first connector”\n\nYou can add a document or link to include in your search\nDanswer also makes it possible to link popular services like Slack and Google docs\nTry to keep your document/website size small enough. If you try to index all of Google your connector will fail.\n\n\n\nIf things don’t work at first, stop the docker process completely and run this:\ndocker compose -f docker-compose.dev.yml -p danswer-stack up -d --pull always --force-recreate\n\nSometimes this is required to run a few times to make sure the .env file is being used for some reason.\nPotentially restart ollama and docker if that doesn’t work\n\n(bonus) For anyone running MacOS 13 or higher, open the link in Safari and from the menu bar, choose File &gt; Add to Dock. This will create a standalone webapp.\n\nMore info here: https://support.apple.com/en-us/104996"
  },
  {
    "objectID": "posts/local-llms/index.html#code-completion-and-generation-using-continue",
    "href": "posts/local-llms/index.html#code-completion-and-generation-using-continue",
    "title": "Setting up local LLMs using ollama",
    "section": "Code completion and generation using Continue",
    "text": "Code completion and generation using Continue\nI haven’t tested this one yet, but it was on the ollama blog and should work well using the local LLMs installed with ollama. Continue acts as your own AI code assistant within your IDE. It works with VScode and JetBrains for now.\nContinue: https://www.continue.dev/\nVScode extension: https://marketplace.visualstudio.com/items?itemName=Continue.continue\nollama blog post: https://ollama.com/blog/continue-code-assistant"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Getting to know ggplot2",
    "section": "",
    "text": "This is a post with executable code. Testing adding text here.\n\nplot(1,1)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Tutorials",
    "section": "",
    "text": "Getting to know ggplot2\n\n\n\n\n\n\n\ncode\n\n\nplotting\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2024\n\n\nElizabeth Leshuk\n\n\n\n\n\n\n  \n\n\n\n\nSetting up local LLMs using ollama\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\nSven Nelson\n\n\n\n\n\n\nNo matching items"
  }
]