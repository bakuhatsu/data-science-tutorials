{
  "hash": "db7678e677bde1690a2d40694f4f01a0",
  "result": {
    "markdown": "---\ntitle: \"Setting up local LLMs using ollama\"\nauthor: \"Sven Nelson\"\ndate: \"2024-06-03\"\ncategories: [news]\nformat:\n  html:\n    code-overflow: wrap\n---\n\n## Getting started in RStudio\n\n-   Brief overview\n-   Projects\n-   Github\n-   (very brief) Quarto/this blog\n\n## Installing local LLMs\n\n### Ollama\n\n1.  Download ollama and install (available for Mac, Linux, and Windows).\n\n    -   <https://ollama.com/>\n\n2.  Run ollama for the first time, you will see a llama icon in your menu bar.\n\n3.  Now you can run your first test with:\n\n    ```bash\n    ollama run llama3\n    ```\n\n    This will download the model `llama3` and start an interactive chat session.\n\n    When you are done, end chat with `\\bye`\n\n    **You can download many other models using this terminal command:**\n\n    | Model              | Parameters | Size  | Download                       |\n    |--------------------|------------|-------|--------------------------------|\n    | Llama 3            | 8B         | 4.7GB | `ollama run llama3`            |\n    | Llama 3            | 70B        | 40GB  | `ollama run llama3:70b`        |\n    | Phi 3 Mini         | 3.8B       | 2.3GB | `ollama run phi3`              |\n    | Phi 3 Medium       | 14B        | 7.9GB | `ollama run phi3:medium`       |\n    | Gemma              | 2B         | 1.4GB | `ollama run gemma:2b`          |\n    | Gemma              | 7B         | 4.8GB | `ollama run gemma:7b`          |\n    | Mistral            | 7B         | 4.1GB | `ollama run mistral`           |\n    | Moondream 2        | 1.4B       | 829MB | `ollama run moondream`         |\n    | Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`       |\n    | Starling           | 7B         | 4.1GB | `ollama run starling-lm`       |\n    | Code Llama         | 7B         | 3.8GB | `ollama run codellama`         |\n    | Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored` |\n    | LLaVA              | 7B         | 4.5GB | `ollama run llava`             |\n    | Solar              | 10.7B      | 6.1GB | `ollama run solar`             |\n\n> Note: You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n\n### Open WebUI graphical chat app for ollama models\n\nIt would be nicer if we had a GUI chat interface to interact with the model and could get code responses in syntax-highlighted code blocks. We can do that and more by installing Open WebUI.\n\nOpen WebUI can also be used with non-local models like ChatGPT and Github Copilot, but we will focus on local LLMs for this tutorial.\n\nOpen WebUI: <https://github.com/open-webui/open-webui>\n\n1.  You will need to have docker installed. The easiest method is to download and install Docker Desktop from here: <https://www.docker.com/products/docker-desktop/>\n\n    -   Docker Desktop is free and can be used for commercial use for companies up to 250 employees OR \\$10 million in yearly revenue.\n\n2.  Once Docker Desktop is installed, run it. This will enable you to use the `docker` command from the terminal.\n\n    -   There are other ways to install docker, you just need to be able to run `docker` and `docker compose` commands.\n\n3.  Install and run Open WebUI with the following command:\n\n\n    ```{default}\n    docker run -d -p 3000:8080 –add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data –name open-webui –restart always ghcr.io/open-webui/open-webui:main\n    ```\n\n\n4.  In a web browser go to [http://localhost:3000](http://localhost:3000/c/e170b94b-c658-4413-9aaa-40cb8bbcdd30)\n\n    -   Open WebUI will load\n\n    -   You can install additional models from the menu\n\n    -   You can save conversation chains\n\n    -   You can select multiple models and ask a question and it will select the best suited model to answer your question.\n\n## Installing Danswer for RAG with ollama models\n\nRAG stands for Retrieval-Augmented Generation. So far our models can answer questions and are great for giving code examples, but the model cannot access a web page or pull information from local documents. With Danswer, we can allow ollama models to do exactly that.\n\nThis install is a little more involved than the last.\n\n1.  \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}